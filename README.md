# nuP_Transfer
Обучение больших нейронных сетей представляет сложность, поскольку мы до конца не осознаем, как изменяется их поведение при увеличении размеров. Исследования в области глубокого обучения, проведённые в таких статьях как: 

https://proceedings.mlr.press/v9/glorot10a.html

https://www.microsoft.com/en-us/research/publication/delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification/

и других, привели к созданию полезных эвристик, которые сейчас активно применяются специалистами. Эти подходы направлены на поддержание стабильности масштабов активаций на этапе инициализации. Тем не менее, после старта обучения эта стабильность нарушается для моделей различной ширины, что проиллюстрировано на левом графике.
 
 В отличие от случайной инициализации, поведение во время обучения гораздо сложнее математически проанализировать. Необходимо было добиться аналогичной согласованности, чтобы по мере увеличения ширины модели изменение масштабов активации во время обучения оставалось согласованным и аналогичным инициализации. 

Решение этой проблемы предложила группа ученых из Microsoft совместно с Open.AI назвав новый метод Maximal Update Parametrization (µP).

Метод μP акцентирует внимание на оптимизации процесса обновления параметров во время обучения. Основная цель заключается в том, чтобы обеспечить максимально эффективные изменения параметров без нарушения стабильности обучения. По сути, μP регулирует масштабы этих обновлений на каждом шаге, предотвращая ситуации, когда они становятся либо чрезмерно малыми, что замедляет обучение, либо слишком большими, что может привести к расхождению модели. μP применяет математический подход, который анализирует влияние каждого обновления на весь процесс обучения. Это позволяет вычислить оптимальный коэффициент масштабирования для всех изменений, гарантирующий, что они окажут максимальное воздействие, но при этом сохранят стабильность модели.
Первый раз на практике метод испытали при обучении архитектуры GPT-3, где гиперпараметры переносились от модели с 40 миллионами параметров к модели с 6,7 миллиардами параметров.

 ![image](https://github.com/user-attachments/assets/68f37f66-3f78-42a2-bee7-1d93df4be649)


(На рисунке изображены два графика, иллюстрирующие различие в поведении масштабов активации нейронной сети в зависимости от её ширины. Левый график демонстрирует ситуацию при использовании стандартной параметризации в PyTorch: уже после первого шага обучения масштабы активаций начинают сильно варьироваться в зависимости от ширины модели. На правом графике, представляющем результаты применения метода µP, видно, что изменение масштабов активаций остаётся постоянным вне зависимости от ширины модели на любом шаге обучения. По оси y отображается изменение масштаба активации сети на одном и том же входном сигнале через 0, 1, 2, 3 и 4 шага обучения, а ось x отражает увеличение ширины модели.)


Моя задача изучить и воспроизвести данный метод.
За основу я взял стандартную архитектуру GPT-2. 
Одну модель я оставил в стоке, во второй head был заменен на специализированный слой. MuReadout это обёртка вокруг nn.Linear, которая масштабирует начальную дисперсию веса с помощью ширины множителя, рассчитанного функцией 
set_base_shapes()
![image](https://github.com/user-attachments/assets/4cc5b300-73f9-450e-a583-4c2ceef2731a)
![image](https://github.com/user-attachments/assets/30fb3c8a-bbbd-45b6-a8b4-4a608e139a10)
При данных настройках Target model не сходится, в то время как моедлька с донорскими настройками показывает отличные результаты.
![image](https://github.com/user-attachments/assets/509e8d26-10b1-4112-8904-4b54a4f04fb1)

![image](https://github.com/user-attachments/assets/00827346-4ea4-489b-b80d-7c487d8b7244)

(донорская моделька)
![image](https://github.com/user-attachments/assets/cd4da70f-58ff-4bf5-9d70-dcc1bacf6a44)




 
 


